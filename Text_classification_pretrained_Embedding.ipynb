{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMN4G0NVU224Q/nefHlTVgs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/memoandrew/Sinandah/blob/main/Text_classification_pretrained_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzstinLfaLV0"
      },
      "outputs": [],
      "source": [
        "#IMPORT THE NECESSARY LIBRARIES\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM,Dense,Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DOWNLOAD THE PRETRAINED EMBEDDING\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip   -d glove.6B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1AgUJZQc75B",
        "outputId": "50744d81-b4b4-4a25-a16f-8dd860578430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-03 08:53:34--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-05-03 08:53:34--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-05-03 08:53:34--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.08MB/s    in 2m 39s  \n",
            "\n",
            "2024-05-03 08:56:14 (5.16 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD THE AG-NEWS DATASET AND SPLIT INTO TRAINING AND TESTING SETS\n",
        "\n",
        "dataset, info = tfds.load('ag_news_subset',with_info = True, as_supervised = True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "metadata": {
        "id": "sKVwOcOiemUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TOKENIZE AND SEQUENCE THE TRAINING SET\n",
        "tokenizer = Tokenizer(num_words = 20000, oov_token = \"<OOV>\")\n",
        "train_texts = [x[0].numpy().decode('utf-8') for x in train_dataset]\n",
        "\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "train_sequences = pad_sequences(train_sequences, padding = 'post')\n",
        "max_length = train_sequences.shape[1]"
      ],
      "metadata": {
        "id": "ShxFWeElS-eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PREPROCESS THE TEST DATA\n",
        "#fit_on-texts IS NOT APPLIED ON TEST SET TO ENSURE THAT THE TOKENIZER REMAINS THE SAME AS THE TRAINING SET\n",
        "\n",
        "test_texts = [x[0].numpy().decode('utf-8') for x in test_dataset]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "train_sequences = pad_sequences(test_sequences, padding = 'post', maxlen = max_length)\n",
        "\n"
      ],
      "metadata": {
        "id": "E-h_Nn98U19U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#THE EMBEDDING PARAMETERS\n",
        "#I DEFINE THE VOCABULARY AND SET THE DIMENSIONALITY OF MY EMBEDDING TO 50 AS I'LL BE WORKING WITH THE 50d PRETRAINED EMBEDDING\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 50"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fjSAcl3mYUP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#APPLY THE PRETRAINED WORD EMBEDDING\n",
        "#I ACCESS THE glove.6B.txt FILE AND READ IT LINE BY LINE, EACH LINE CONTAINING A WORD AND ITS CORRESPONDING EMBEDDING\n",
        "#I CROSS-MATCH   WORDS IN THE GLOVE FILE WITH THOSE IN MY VOCABULARY, CONSTRUCTED WITH THE KERAS TOKENIZER\n",
        "#IF THERE'S A MATCH, I TAKE THE CORRESPONDING WORD INDEX FROM MY OWN VOCABULARY AND UPDATE MY INITIALLY ZERO-INITIALIZED\n",
        "#EMBEDDING MATRIX AT THAT INDEX WITH THE GLOVE EMBEDDINGS\n",
        "#WORDS THAT DON'T MATCH WILL REMAIN AS ZERO VECTORS IN THE MATRIX\n",
        "#I WILL USE THIS EMBEDDING MATRIX TO INITIALIZE THE WEIGHT OF MY EMBEDDING LAYER\n",
        "      #USE THE FILE PATH TO THE 50d EMBEDDING\n",
        "#DOWNLOAD GLOVE EMBEDDINGS AND PREPARE EMBEDDING MATRIX\n",
        "\n",
        "with open('/content/glove.6B/glove.6B.txt.50d', 'r', encoding = 'utf-8') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    if word in tokenizer.word_index:\n",
        "      idx = tokenizer.word_index[word]\n",
        "      embedding_matrix[idx] = np.array(values[:1], dtype = np.float32)"
      ],
      "metadata": {
        "id": "zEgDn5Hla7Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BUILD, COMPILE AND TRAIN THE LSTM MODEL\n",
        "#I LEVERAGE THE PRETRAINED EMBEDDING MATRIX TO INITIALIZE THE EMBEDDING LAYER\n",
        "#I SET TRAINABLE PARAMETERS TO FALSE TO ENSURE THE WEIGHTS REMAIN UNCHANGED\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_lenght = max_length, weights = [embedding_matrix], trainable = False),\n",
        "    LSTM(32, return_sequences = True),\n",
        "    LSTM(32),\n",
        "    Dense(64, activation = 'relu'),\n",
        "    Dense(4, activation = 'softmax')\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "BL42_69cfQXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#COMPILE AND FIT THE MODEL\n",
        "\n",
        "model_lstm.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "TwGnCfEshgO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CONVER LABELS TO ONE-HOT ENCODING\n",
        "\n",
        "train_labels = tf.keras.utils.to_categorical(\n",
        "    [label.numpy() for _, label in test_dataset]\n",
        ")\n",
        "\n",
        "model_lstm.fit(train_sequences, train_labels, epochs = 10, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "WZN1dWwKiEmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL EVALUATION\n",
        "\n",
        "loss, accuracy = model_lstm.evaluate(test_sequences, test_labels)\n",
        "print(\"Loss:\" loss)\n",
        "print(\"Accuracy:\" accuracy)"
      ],
      "metadata": {
        "id": "R3mS8nAOjJvg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}